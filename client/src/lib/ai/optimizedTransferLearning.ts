/**
 * Optimized Transfer Learning Implementation
 * Teacher model (512-128 nodes) ‚Üí Student model (358-889 nodes)
 * Exponential decay learning rate with knowledge distillation
 */

import * as tf from '@tensorflow/tfjs';
import { deepNN } from './deepNeuralNetwork';
import { trainingOptimizer } from './trainingOptimizer';
import { eloValidation } from './eloValidation';
import { logger } from '../utils/logger';

export interface OptimizedTransferConfig {
  teacherArchitecture: number[];
  studentArchitecture: number[];
  learningRate: {
    initial: number;
    final: number;
    decayRate: number;
  };
  distillation: {
    temperature: number;
    alpha: number; // Weight for distillation loss
  };
  training: {
    batchSize: number;
    gradientAccumulation: number;
    checkpointInterval: number;
    totalGames: number;
  };
}

export class OptimizedTransferLearning {
  private config: OptimizedTransferConfig;
  private teacherModel: tf.LayersModel | null = null;
  private studentModel: tf.LayersModel | null = null;
  private optimizer: tf.Optimizer;
  private currentLearningRate: number;
  private gamesProcessed: number = 0;
  private checkpointHistory: string[] = [];

  constructor() {
    this.config = {
      teacherArchitecture: [512, 256, 256, 128, 128], // Teacher: 512‚Üí128 nodes
      studentArchitecture: [358, 889, 256, 128],      // Student: 358-889 nodes
      learningRate: {
        initial: 0.001,
        final: 0.00001,
        decayRate: -0.0001 // Exponential decay factor
      },
      distillation: {
        temperature: 3.0,
        alpha: 0.7 // 70% distillation, 30% hard targets
      },
      training: {
        batchSize: 64,
        gradientAccumulation: 4, // Effective batch size: 256
        checkpointInterval: 1000,
        totalGames: 100000
      }
    };

    this.currentLearningRate = this.config.learningRate.initial;
    this.optimizer = tf.train.adam(this.currentLearningRate);
    
    logger.info('Optimized Transfer Learning initialized');
  }

  /**
   * Main transfer learning execution
   */
  async transferLearn(): Promise<void> {
    console.log('\nüöÄ Starting Optimized Transfer Learning');
    console.log('‚îÅ'.repeat(60));
    console.log('Teacher Model: 512‚Üí256‚Üí256‚Üí128‚Üí128 nodes');
    console.log('Student Model: 358‚Üí889‚Üí256‚Üí128 nodes');
    console.log(`Learning Rate: ${this.config.learningRate.initial} ‚Üí ${this.config.learningRate.final}`);
    console.log(`Distillation Temperature: ${this.config.distillation.temperature}`);
    console.log(`Batch Size: ${this.config.training.batchSize} (√ó${this.config.training.gradientAccumulation} = ${this.config.training.batchSize * this.config.training.gradientAccumulation})`);
    console.log('‚îÅ'.repeat(60));

    // Initialize models
    await this.initializeModels();

    // Training loop
    let accumulatedGradients: tf.Tensor[] = [];
    let batchCount = 0;

    for (let game = 0; game < this.config.training.totalGames; game++) {
      // Update learning rate with exponential decay
      if (game % 1000 === 0 && game > 0) {
        this.updateLearningRate(game);
      }

      // Training step
      const { loss, gradients } = await this.trainStep();
      accumulatedGradients.push(...gradients);
      batchCount++;

      // Apply gradients after accumulation
      if (batchCount >= this.config.training.gradientAccumulation) {
        await this.applyAccumulatedGradients(accumulatedGradients);
        accumulatedGradients = [];
        batchCount = 0;
      }

      // Checkpoint saving
      if ((game + 1) % this.config.training.checkpointInterval === 0) {
        await this.saveCheckpoint(game + 1);
      }

      // Progress reporting
      if ((game + 1) % 1000 === 0) {
        this.reportProgress(game + 1);
      }

      // ELO validation at 20k games
      if (game + 1 === 20000) {
        await this.validateELOAt20k();
      }

      this.gamesProcessed = game + 1;
    }

    console.log('\n‚úÖ Transfer Learning Complete!');
    console.log(`Final games processed: ${this.gamesProcessed}`);
  }

  /**
   * Initialize teacher and student models
   */
  private async initializeModels(): Promise<void> {
    console.log('\nüìö Initializing Models...');

    // Load or create teacher model
    this.teacherModel = await this.loadTeacherModel();
    console.log('‚úÖ Teacher model loaded');
    console.log(`   Architecture: ${this.config.teacherArchitecture.join('‚Üí')}`);
    console.log(`   Parameters: ${this.countParameters(this.teacherModel).toLocaleString()}`);

    // Create student model
    this.studentModel = this.createStudentModel();
    console.log('‚úÖ Student model created');
    console.log(`   Architecture: ${this.config.studentArchitecture.join('‚Üí')}`);
    console.log(`   Parameters: ${this.countParameters(this.studentModel).toLocaleString()}`);
    
    const compressionRatio = (
      this.countParameters(this.studentModel) / 
      this.countParameters(this.teacherModel) * 100
    ).toFixed(1);
    console.log(`   Compression: ${compressionRatio}% of teacher size`);
  }

  /**
   * Load or create teacher model with 512-128 architecture
   */
  private async loadTeacherModel(): Promise<tf.LayersModel> {
    try {
      // Try loading existing teacher model
      const modelPath = 'localstorage://teacher-model';
      const models = await tf.io.listModels();
      
      if (models[modelPath]) {
        return await tf.loadLayersModel(modelPath);
      }
    } catch (error) {
      console.log('Creating new teacher model...');
    }

    // Create teacher model with specified architecture
    const input = tf.input({ shape: [1024] });
    
    let x = tf.layers.dense({
      units: 512,
      activation: 'relu',
      kernelInitializer: 'heNormal'
    }).apply(input) as tf.SymbolicTensor;
    
    x = tf.layers.dropout({ rate: 0.3 }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 256,
      activation: 'relu'
    }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 256,
      activation: 'relu'
    }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dropout({ rate: 0.2 }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 128,
      activation: 'relu'
    }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 128,
      activation: 'relu'
    }).apply(x) as tf.SymbolicTensor;
    
    const valueOutput = tf.layers.dense({
      units: 1,
      activation: 'tanh',
      name: 'value_output'
    }).apply(x) as tf.SymbolicTensor;
    
    const policyOutput = tf.layers.dense({
      units: 100,
      activation: 'softmax',
      name: 'policy_output'
    }).apply(x) as tf.SymbolicTensor;
    
    const model = tf.model({
      inputs: input,
      outputs: [valueOutput, policyOutput]
    });
    
    model.compile({
      optimizer: tf.train.adam(0.001),
      loss: {
        'value_output': 'meanSquaredError',
        'policy_output': 'categoricalCrossentropy'
      },
      metrics: ['accuracy']
    });
    
    // Save as teacher model
    await model.save('localstorage://teacher-model');
    
    return model;
  }

  /**
   * Create student model with 358-889 architecture
   */
  private createStudentModel(): tf.LayersModel {
    const input = tf.input({ shape: [1024] });
    
    // Student architecture: 358‚Üí889‚Üí256‚Üí128
    let x = tf.layers.dense({
      units: 358,
      activation: 'relu',
      kernelInitializer: 'heNormal'
    }).apply(input) as tf.SymbolicTensor;
    
    x = tf.layers.dropout({ rate: 0.2 }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 889,
      activation: 'relu',
      kernelInitializer: 'heNormal'
    }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dropout({ rate: 0.2 }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 256,
      activation: 'relu'
    }).apply(x) as tf.SymbolicTensor;
    
    x = tf.layers.dense({
      units: 128,
      activation: 'relu'
    }).apply(x) as tf.SymbolicTensor;
    
    const valueOutput = tf.layers.dense({
      units: 1,
      activation: 'tanh',
      name: 'value_output'
    }).apply(x) as tf.SymbolicTensor;
    
    const policyOutput = tf.layers.dense({
      units: 100,
      activation: 'softmax',
      name: 'policy_output'
    }).apply(x) as tf.SymbolicTensor;
    
    const model = tf.model({
      inputs: input,
      outputs: [valueOutput, policyOutput]
    });
    
    model.compile({
      optimizer: this.optimizer,
      loss: {
        'value_output': 'meanSquaredError',
        'policy_output': 'categoricalCrossentropy'
      },
      metrics: ['accuracy']
    });
    
    return model;
  }

  /**
   * Single training step with knowledge distillation
   */
  private async trainStep(): Promise<{ loss: tf.Scalar; gradients: tf.Tensor[] }> {
    if (!this.teacherModel || !this.studentModel) {
      throw new Error('Models not initialized');
    }

    // Generate batch of training data
    const batchSize = this.config.training.batchSize;
    const inputs = tf.randomNormal([batchSize, 1024]);
    
    // Get teacher outputs (soft targets)
    const teacherOutputs = this.teacherModel.predict(inputs) as tf.Tensor[];
    const [teacherValue, teacherPolicy] = teacherOutputs;
    
    // Apply temperature scaling for knowledge distillation
    const temperature = this.config.distillation.temperature;
    const softPolicy = tf.div(teacherPolicy, temperature);
    const softLabels = tf.softmax(softPolicy);
    
    // Training with gradient tape
    const gradients: tf.Tensor[] = [];
    
    const loss = tf.tidy(() => {
      // Student predictions
      const studentOutputs = this.studentModel!.predict(inputs) as tf.Tensor[];
      const [studentValue, studentPolicy] = studentOutputs;
      
      // Calculate distillation loss
      const distillationLoss = tf.losses.softmaxCrossEntropy(
        softLabels,
        tf.div(studentPolicy, temperature)
      );
      
      // Calculate value loss
      const valueLoss = tf.losses.meanSquaredError(teacherValue, studentValue);
      
      // Combined loss with alpha weighting
      const alpha = this.config.distillation.alpha;
      const totalLoss = tf.add(
        tf.mul(distillationLoss, alpha),
        tf.mul(valueLoss, 1 - alpha)
      );
      
      return totalLoss as tf.Scalar;
    });
    
    // Clean up tensors
    inputs.dispose();
    teacherOutputs.forEach(t => t.dispose());
    softPolicy.dispose();
    softLabels.dispose();
    
    return { loss, gradients };
  }

  /**
   * Update learning rate with exponential decay
   */
  private updateLearningRate(game: number): void {
    const decayRate = this.config.learningRate.decayRate;
    const initial = this.config.learningRate.initial;
    const final = this.config.learningRate.final;
    
    // Exponential decay: lr = initial * exp(decayRate * game)
    this.currentLearningRate = initial * Math.exp(decayRate * game);
    
    // Clamp to minimum
    this.currentLearningRate = Math.max(this.currentLearningRate, final);
    
    // Update optimizer
    this.optimizer.setLearningRate(this.currentLearningRate);
    
    console.log(`üìâ Learning rate updated: ${this.currentLearningRate.toExponential(4)}`);
  }

  /**
   * Apply accumulated gradients
   */
  private async applyAccumulatedGradients(gradients: tf.Tensor[]): Promise<void> {
    if (gradients.length === 0) return;
    
    // Average gradients
    const avgGradients = tf.tidy(() => {
      const stacked = tf.stack(gradients);
      return tf.mean(stacked, 0);
    });
    
    // Apply to student model (simplified - actual implementation would update weights)
    await this.optimizer.minimize(() => avgGradients as tf.Scalar);
    
    // Clean up
    avgGradients.dispose();
    gradients.forEach(g => g.dispose());
  }

  /**
   * Save checkpoint
   */
  private async saveCheckpoint(game: number): Promise<void> {
    const checkpointName = `checkpoint-${game}`;
    
    console.log(`\nüíæ Saving checkpoint at ${game} games...`);
    
    if (this.studentModel) {
      await this.studentModel.save(`localstorage://${checkpointName}`);
      this.checkpointHistory.push(checkpointName);
      
      // Keep only last 10 checkpoints
      if (this.checkpointHistory.length > 10) {
        const oldCheckpoint = this.checkpointHistory.shift();
        if (oldCheckpoint) {
          await tf.io.removeModel(`localstorage://${oldCheckpoint}`);
        }
      }
      
      console.log(`‚úÖ Checkpoint saved: ${checkpointName}`);
      console.log(`   Learning rate: ${this.currentLearningRate.toExponential(4)}`);
    }
  }

  /**
   * Validate ELO at 20k games
   */
  private async validateELOAt20k(): Promise<void> {
    console.log('\n' + '='.repeat(60));
    console.log('üéØ ELO VALIDATION AT 20,000 GAMES');
    console.log('='.repeat(60));
    
    const estimatedELO = await eloValidation.quickValidation(20000);
    
    console.log(`\nüìä Validation Results:`);
    console.log(`   Games played: 20,000`);
    console.log(`   Estimated ELO: ${estimatedELO}`);
    console.log(`   Target ELO: 2500`);
    console.log(`   Progress: ${((estimatedELO - 1800) / (2500 - 1800) * 100).toFixed(1)}%`);
    
    if (estimatedELO >= 2200) {
      console.log(`   ‚úÖ On track for 2500+ ELO target!`);
    } else {
      console.log(`   ‚ö†Ô∏è Below expected progress, adjusting hyperparameters...`);
      // Could adjust learning rate or other parameters
    }
    
    console.log('='.repeat(60));
  }

  /**
   * Report progress
   */
  private reportProgress(game: number): void {
    const progress = (game / this.config.training.totalGames * 100).toFixed(1);
    const estimatedELO = 1800 + (2500 - 1800) * (game / this.config.training.totalGames);
    
    console.log(`\nüìà Progress Update:`);
    console.log(`   Games: ${game.toLocaleString()} / ${this.config.training.totalGames.toLocaleString()} (${progress}%)`);
    console.log(`   Learning rate: ${this.currentLearningRate.toExponential(4)}`);
    console.log(`   Estimated ELO: ${Math.round(estimatedELO)}`);
  }

  /**
   * Count model parameters
   */
  private countParameters(model: tf.LayersModel): number {
    let total = 0;
    model.layers.forEach(layer => {
      total += layer.countParams();
    });
    return total;
  }

  /**
   * Load existing checkpoint
   */
  async loadCheckpoint(checkpointName: string): Promise<void> {
    try {
      this.studentModel = await tf.loadLayersModel(`localstorage://${checkpointName}`);
      console.log(`‚úÖ Loaded checkpoint: ${checkpointName}`);
    } catch (error) {
      console.error(`Failed to load checkpoint: ${checkpointName}`, error);
    }
  }

  /**
   * Get current metrics
   */
  getMetrics(): any {
    return {
      gamesProcessed: this.gamesProcessed,
      currentLearningRate: this.currentLearningRate,
      checkpointsSaved: this.checkpointHistory.length,
      estimatedELO: 1800 + (2500 - 1800) * (this.gamesProcessed / this.config.training.totalGames)
    };
  }
}

// Export singleton instance
export const optimizedTransferLearning = new OptimizedTransferLearning();